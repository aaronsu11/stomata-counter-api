{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IifoEynGbBSX"
   },
   "source": [
    "Author: Hiranya Jayakody. April 2020.\n",
    "\n",
    "Code developed for Smart Robotics Viticulture Group, UNSW, Sydney.\n",
    "\n",
    "Neural Network based on Matterport implementation of Mask-RCNN at https://github.com/matterport/Mask_RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qCvl0TpVhWai"
   },
   "source": [
    "### PART 1.1: Install Mask-RCNN repo from Matterport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W4Ndsg-3bIjN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Projects\\General\\Python\\stomata_counter\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/matterport/Mask_RCNN.git\n",
    "!pip install -r 'Mask_RCNN/requirements.txt'\n",
    "!cd Mask_RCNN ; python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-6Ga523wbcD2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mask-rcnn\n",
      "Version: 2.1\n",
      "Summary: Mask R-CNN for object detection and instance segmentation\n",
      "Home-page: https://github.com/matterport/Mask_RCNN\n",
      "Author: Matterport\n",
      "Author-email: waleed.abdulla@gmail.com\n",
      "License: MIT\n",
      "Location: d:\\projects\\web\\back-end\\stomata-counter-api\\env\\lib\\site-packages\\mask_rcnn-2.1-py3.7.egg\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show mask-rcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ArgHtsc8beX-"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PocPNLe3biAE"
   },
   "outputs": [],
   "source": [
    "!pip install keras==2.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ds-jzLO4wNWm"
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python==3.4.9.31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1.2: Set-up workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CWD = os.getcwd()\n",
    "STOMATA_WEIGHTS_PATH = os.path.join(CWD,'weights/2020_mask_rcnn_stomata_51.h5') \n",
    "WEIGHT_FILE_NAME = 'stomata'\n",
    "CLASS_NAME = 'stomata'\n",
    "DATASET_DIR = os.path.join(CWD,'images/')\n",
    "INPUT_IMG_DIR = os.path.join(DATASET_DIR,'test/') \n",
    "OUTPUT_IMG_DIR = os.path.join(DATASET_DIR,'results/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HAUPaJ0cfyQe"
   },
   "outputs": [],
   "source": [
    "#mount your google drive if necessary\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 3: AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import boto3\n",
    "\n",
    "# defining s3 bucket object\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket_name = \"ainz11-test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog.jpg\n",
      "meme.jpg\n",
      "stomata1.jpg\n"
     ]
    }
   ],
   "source": [
    "# list all file in bucket\n",
    "response = s3.list_objects_v2(\n",
    "    Bucket=bucket_name\n",
    ")\n",
    "for obj in response.get('Contents', None):\n",
    "    print(obj.get('Key', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# start_time = time.time()\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# fetching object from bucket\n",
    "file_obj = s3.get_object(Bucket=bucket_name, Key=\"stomata1.jpg\")\n",
    "# reading the file content in bytes\n",
    "file_content = file_obj[\"Body\"].read()\n",
    "\n",
    "# creating 1D array from bytes data range between[0,255]\n",
    "# np_array = np.frombuffer(file_content, np.uint8)\n",
    "# decoding array\n",
    "# image_np = cv2.imdecode(np_array, cv2.IMREAD_COLOR)\n",
    "# -> in one line:\n",
    "image_np = cv2.imdecode(np.asarray(bytearray(file_content)), cv2.IMREAD_COLOR)\n",
    "\n",
    "# converting image from RGB to Grayscale\n",
    "# gray = cv2.cvtColor(image_np, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# # saving image to tmp (writable) directory\n",
    "# cv2.imwrite(\"gray_obj.jpg\", gray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.6653015613555908 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Using boto3 \"Resource\" --> slower\n",
    "# s3 = boto3.resource('s3', region_name='us-east-2')\n",
    "# bucket = s3.Bucket(bucket_name)\n",
    "# img = bucket.Object(\"stomata1.jpg\").get().get('Body').read()\n",
    "# image_np = cv2.imdecode(np.asarray(bytearray(img)), cv2.IMREAD_COLOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'FB30DA2FF7B05F70',\n",
       "  'HostId': '25LUR5rYjDIYB1Uu/f7auFYI1Jbxr9KHixot2GNaAscIICf9ft7r3M+EQylrqp6eC9GmucwC7ZQ=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '25LUR5rYjDIYB1Uu/f7auFYI1Jbxr9KHixot2GNaAscIICf9ft7r3M+EQylrqp6eC9GmucwC7ZQ=',\n",
       "   'x-amz-request-id': 'FB30DA2FF7B05F70',\n",
       "   'date': 'Mon, 19 Oct 2020 06:13:10 GMT',\n",
       "   'etag': '\"693f053ec58adb36f7e7eef985406441\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"693f053ec58adb36f7e7eef985406441\"'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uploading converted image to S3 bucket\n",
    "# s3.put_object(Bucket=bucket_name, Key=\"grayscale.jpg\", Body=open(\"gray_obj.jpg\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GnePMb0WbJOS"
   },
   "source": [
    "### PART 2: Set-up Mask-RCNN for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3r2zo_YlEWIy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# restart runtime if 'No module named 'mrcnn'' error occurs\n",
    "\n",
    "import cv2\n",
    "import glob\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import skimage.draw\n",
    "import imutils\n",
    "import imgaug\n",
    "import statistics as st\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import visualize\n",
    "from mrcnn import model as modellib, utils\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNxWWbIbbPBV"
   },
   "outputs": [],
   "source": [
    "#create config for inference\n",
    "class InferenceConfig(Config):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    NAME = CLASS_NAME #provide a suitable name\n",
    "    NUM_CLASSES = 1+1 #background+number of classes\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    RPN_ANCHOR_SCALES = (12,24,48,96,192) #anchor box scales for the application\n",
    "    DETECTION_MIN_CONFIDENCE = 0.6 #set min confidence threshold\n",
    "    DETECTION_MAX_INSTANCES = 500\n",
    "    POST_NMS_ROIS_INFERENCE = 10000\n",
    "    IMAGE_MAX_DIM = 1024\n",
    "    IMAGE_MIN_DIM = 800\n",
    "    MEAN_PIXEL = np.array([133.774, 133.774, 133.774]) #DEFAULT VALUES FOR STOMATA MODEL. Change as necessary. #matterport takes the input as RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p3dmNYfGeA6Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        500\n",
      "DETECTION_MIN_CONFIDENCE       0.6\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                14\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [133.774 133.774 133.774]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           stomata\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        10000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (12, 24, 48, 96, 192)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From d:\\projects\\web\\back-end\\stomata-counter-api\\env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\projects\\web\\back-end\\stomata-counter-api\\env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\projects\\web\\back-end\\stomata-counter-api\\env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\projects\\web\\back-end\\stomata-counter-api\\env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3661: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\projects\\web\\back-end\\stomata-counter-api\\env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1944: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\projects\\web\\back-end\\stomata-counter-api\\env\\lib\\site-packages\\mask_rcnn-2.1-py3.7.egg\\mrcnn\\model.py:341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\projects\\web\\back-end\\stomata-counter-api\\env\\lib\\site-packages\\mask_rcnn-2.1-py3.7.egg\\mrcnn\\model.py:399: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From d:\\projects\\web\\back-end\\stomata-counter-api\\env\\lib\\site-packages\\mask_rcnn-2.1-py3.7.egg\\mrcnn\\model.py:423: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n",
      "WARNING:tensorflow:From d:\\projects\\web\\back-end\\stomata-counter-api\\env\\lib\\site-packages\\mask_rcnn-2.1-py3.7.egg\\mrcnn\\model.py:720: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\projects\\web\\back-end\\stomata-counter-api\\env\\lib\\site-packages\\mask_rcnn-2.1-py3.7.egg\\mrcnn\\model.py:722: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\projects\\web\\back-end\\stomata-counter-api\\env\\lib\\site-packages\\mask_rcnn-2.1-py3.7.egg\\mrcnn\\model.py:772: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "Loading weights from  D:\\Projects\\Web\\Back-end\\stomata-counter-api\\weights/2020_mask_rcnn_stomata_51.h5\n",
      "WARNING:tensorflow:From d:\\projects\\web\\back-end\\stomata-counter-api\\env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\projects\\web\\back-end\\stomata-counter-api\\env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:178: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\projects\\web\\back-end\\stomata-counter-api\\env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:180: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\projects\\web\\back-end\\stomata-counter-api\\env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:184: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\projects\\web\\back-end\\stomata-counter-api\\env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:193: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\projects\\web\\back-end\\stomata-counter-api\\env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:200: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create inference object\n",
    "inference_config = InferenceConfig()\n",
    "inference_config.display()\n",
    "\n",
    "# Load the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", config=inference_config, model_dir=CWD)\n",
    "model_path = STOMATA_WEIGHTS_PATH #os.path.join(CWD,'mask_rcnn_stomata.h5')\n",
    "\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-9eVILPvbP_R"
   },
   "source": [
    "### PART 3: Apply model to identify stomata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dBZk-wAgFfsf"
   },
   "source": [
    "3.1 Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vIcMsbB2eivy"
   },
   "outputs": [],
   "source": [
    "# Statistical filter\n",
    "\n",
    "def stomata_filter(r_pd_):\n",
    "    #stomata filer: This code filters out stomata like shapes which are of wrong size, using confidence measures.\n",
    "    high_scores = r_pd_[\"scores\"][r_pd_[\"scores\"]>0.90]\n",
    "            \n",
    "    if len(high_scores) > 0:\n",
    "        percentile_thres = np.min(high_scores)\n",
    "    else:\n",
    "        percentile_thres = np.percentile(r_pd_[\"scores\"], 95) #st.median(r_pd_[\"scores\"])\n",
    "    \n",
    "    high_conf_areas = r_pd_[\"areas\"][r_pd_[\"scores\"]>=percentile_thres] #conf_threshold\n",
    "    high_conf_scores = r_pd_[\"scores\"][r_pd_[\"scores\"]>=percentile_thres]\n",
    "    \n",
    "    high_conf_avg_area = st.mean(high_conf_areas)\n",
    "    \n",
    "    above_avg = high_conf_areas[high_conf_areas>=high_conf_avg_area]\n",
    "    below_avg = high_conf_areas[high_conf_areas<high_conf_avg_area]\n",
    "    \n",
    "    above_avg_conf = high_conf_scores[high_conf_areas>=high_conf_avg_area]\n",
    "    below_avg_conf = high_conf_scores[high_conf_areas<high_conf_avg_area]\n",
    "    \n",
    "    #based on data length\n",
    "    if len(above_avg) >= len(below_avg):\n",
    "        optimal_area = np.percentile(above_avg, 50) #(st.mean(above_avg)+np.max(above_avg))/2.0 #can we use percentie \n",
    "        st_size = 'LARGE'\n",
    "    else:\n",
    "        #smaller elements may not be stomata, so check for their overall confidence with respect to the confidence of larger areas\n",
    "        if np.mean(above_avg_conf) >= np.mean(below_avg_conf) and len(above_avg) > 1:\n",
    "            optimal_area = np.percentile(above_avg, 50)\n",
    "            st_size = 'LARGE'\n",
    "        elif len(above_avg) <=1 and np.max(above_avg_conf) > 0.985:\n",
    "            optimal_area = np.percentile(above_avg, 50)\n",
    "            st_size = 'LARGE'\n",
    "        else:\n",
    "            optimal_area = np.percentile(below_avg, 75)\n",
    "            st_size = 'SMALL'\n",
    "       \n",
    "    if st_size == 'LARGE':\n",
    "        indices_ = r_pd_[\"scores\"][np.logical_and(r_pd_[\"areas\"]> (optimal_area*0.55),r_pd_[\"areas\"]<1.5*optimal_area )].index.values.astype(int)\n",
    "    else:\n",
    "        indices_ = r_pd_[\"scores\"][np.logical_and(r_pd_[\"areas\"]> (optimal_area*0.65),r_pd_[\"areas\"]<1.5*optimal_area )].index.values.astype(int)\n",
    "\n",
    "    return indices_\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IB5HnD4qe3_H"
   },
   "outputs": [],
   "source": [
    "# Other supporting functions\n",
    "\n",
    "def get_filename(string_):\n",
    "    #get image filename    \n",
    "    start = string_.find('test/')+5\n",
    "    end = string_.find('.jpg',start)\n",
    "    filename = string_[start:end]\n",
    "    return filename\n",
    "\n",
    "def hisEqulColor(img):\n",
    "    #contrast limited histogram equalisation\n",
    "    ycrcb=cv2.cvtColor(img,cv2.COLOR_BGR2YCR_CB)\n",
    "    channels=cv2.split(ycrcb)\n",
    "    clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(25,25))\n",
    "    channels[0] = clahe.apply(channels[0])\n",
    "    cv2.merge(channels,ycrcb)\n",
    "    cv2.cvtColor(ycrcb,cv2.COLOR_YCR_CB2BGR,img)\n",
    "    return img\n",
    "\n",
    "def sharpenColor(img):\n",
    "    #sharpen image\n",
    "    kernel_sharpening = np.array([[-1,-1,-1], \n",
    "                              [-1, 9,-1],\n",
    "                              [-1,-1,-1]])\n",
    "    img = cv2.filter2D(img, -1, kernel_sharpening)\n",
    "    return img\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L0mZ7Zs9eTYc"
   },
   "source": [
    "### PART 3A: Test on a single image ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TaqTIZmvLRUB"
   },
   "source": [
    "Please Refer to Part 3B and remove the loop for the folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bdDGiGfvebtX"
   },
   "source": [
    "### PART 3B: Test on an image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a8jKDCt9eh6K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.9\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = os.path.join(INPUT_IMG_DIR,'*jpg')\n",
    "# files = glob.glob(DATA_PATH)\n",
    "files = [image_np]\n",
    "\n",
    "stomata_data = pd.DataFrame(columns=['filename','num_stomata','scores','areas'])\n",
    "\n",
    "print(cv2. __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HeeX8Brufda_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making predictions with Mask R-CNN...\n",
      "Processing 1 images\n",
      "image                    shape: (771, 1024, 3)        min:   23.00000  max:  224.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min: -133.77400  max:   90.22600  float64\n",
      "image_metas              shape: (1, 14)               min:    0.00000  max: 1024.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.13271  max:    1.07015  float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\web\\back-end\\stomata-counter-api\\env\\lib\\site-packages\\ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "# for img in files:\n",
    "#     filename = get_filename(img)\n",
    "#     print(filename)\n",
    "#     image = cv2.imread(img)\n",
    "img = files[0]\n",
    "filename = \"test\"\n",
    "image = imutils.resize(img,width=1024)\n",
    "image_original = image\n",
    "image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB) #opnecv uses BGR convention\n",
    "\n",
    "\n",
    "#call histogram equalize function (optional)\n",
    "image = hisEqulColor(image)\n",
    "#image = sharpenColor(image)\n",
    "\n",
    "#convert to grayscale and save as a three channel jpeg then read it back and convert\n",
    "image_gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "cv2.imwrite('current_image.jpg', image_gray)\n",
    "\n",
    "image_new = cv2.imread('current_image.jpg')\n",
    "image = cv2.cvtColor(image_new,cv2.COLOR_BGR2RGB) #opnecv uses BGR convention\n",
    "image = imutils.resize(image,width=1024)\n",
    "\n",
    "#run the image through the model\n",
    "print(\"making predictions with Mask R-CNN...\")\n",
    "r = model.detect([image], verbose=1)[0]\n",
    "\n",
    "#create dataframe for ease of use\n",
    "r_pd = pd.DataFrame(columns=['class_id','scores', 'areas'])\n",
    "\n",
    "#create array to store areas\n",
    "num_stomata = len(r[\"scores\"])\n",
    "\n",
    "r_pd[\"class_ids\"] = r[\"class_ids\"]\n",
    "r_pd[\"scores\"] = r[\"scores\"]\n",
    "\n",
    "#retrieve area values from X,Y coordinates\n",
    "for i in range(0, len(r_pd[\"scores\"])): \n",
    "    (startY,startX,endY,endX) = r[\"rois\"][i]\n",
    "    r_pd[\"areas\"][i] = abs(startY-endY)*abs(startX-endX)\n",
    "\n",
    "\n",
    "#see how many stomata are on the image\n",
    "#1. If there are more than 2 stomata, do the following.\n",
    "#2. get the median score for confidence\n",
    "#3. get the average area for median and above\n",
    "#4. reject areas 90% or less than the average median area\n",
    "\n",
    "if num_stomata == 0:\n",
    "    print(\"no stomata detected\")\n",
    "    stomata_data.loc[counter] = [filename,num_stomata,0]\n",
    "    counter +=1\n",
    "    cv2.imwrite(OUTPUT_IMG_DIR+filename+'_000'+'.jpg', image)\n",
    "    sys.exit()\n",
    "\n",
    "if num_stomata >= 2:\n",
    "\n",
    "    indices = stomata_filter(r_pd)\n",
    "    #indices = r_pd[\"scores\"][:].index.values.astype(int) #this ignores the statistical filter\n",
    "\n",
    "else: \n",
    "    indices = [0]\n",
    "\n",
    "print(indices)\n",
    "\n",
    "# loop over of the detected object's bounding boxes and masks\n",
    "areas = []\n",
    "for i in range(0, len(indices)):\n",
    "    classID = r[\"class_ids\"][indices[i]]\n",
    "    mask = r[\"masks\"][:, :, indices[i]]\n",
    "    color = [0,0,0]\n",
    "    areas.append(np.sum(mask == True))\n",
    "\n",
    "    #uncomment to visualize the pixel-wise mask of the object\n",
    "    #image = visualize.apply_mask(image, mask, color, alpha=0.5)\n",
    "\n",
    "    #visualize contours\n",
    "    mask[mask ==True] = 1\n",
    "    mask[mask == False] = 0\n",
    "    mask = mask.astype(np.uint8)\n",
    "    mask,contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    cv2.drawContours(image_original,contours, 0, [0,255,0], 4)\n",
    "\n",
    "image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n",
    "\n",
    "for i in range(0,len(indices)):\n",
    "    (startY,startX,endY,endX) = r[\"rois\"][indices[i]]\n",
    "    classID = r[\"class_ids\"][indices[i]]\n",
    "    label = classID\n",
    "    score = r[\"scores\"][indices[i]]\n",
    "    color = [255,0,0]\n",
    "\n",
    "    #uncomment to draw bounding box around stomata\n",
    "    #cv2.rectangle(image_original,(startX,startY),(endX,endY),color,2)\n",
    "\n",
    "    #uncomment to print confidence value\n",
    "    #text = \"{}: {:.3f}\".format(label,score)\n",
    "    #y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "    #cv2.putText(image_original,text,(startX,y),cv2.FONT_HERSHEY_SIMPLEX,0.8,color,2)\n",
    "\n",
    "id_str= str(len(indices))\n",
    "stomata_data.loc[counter] = [filename,len(indices),r[\"scores\"][indices],areas]\n",
    "counter +=1\n",
    "cv2.imwrite(OUTPUT_IMG_DIR+filename+'_'+id_str.zfill(3)+'.jpg', image_original)\n",
    "    \n",
    "\n",
    "stomata_data.to_csv(OUTPUT_IMG_DIR+'results.csv',encoding='utf-8', index=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: class_ids, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "print(r_pd[\"class_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNu+joZ0LDnUch4dkhh0Us7",
   "collapsed_sections": [],
   "name": "stomata_inference.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
